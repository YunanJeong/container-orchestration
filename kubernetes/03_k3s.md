# K3s

## Intallation

- [요구사항(포트, 메모리 등)](https://docs.k3s.io/installation/requirements)
- [K3s 시작, 설치](https://docs.k3s.io/quick-start)

### K3s Server 설치 (Controlplane)

노드 1개일 땐 K3s Server만 설치하면 됨

```sh
# K3s 설치
curl -sfL https://get.k3s.io | sh -

# K3s 설치 (특정 버전, https://github.com/k3s-io/k3s/releases)
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION={version} sh -

# K3s 설치 (K3s의 컨테이너 런타임으로 기설치된 docker를 사용)
curl -sfL https://get.k3s.io | sh -s - --docker
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION={version} sh -s - --docker
```

### 설치 후 설정

```sh
# k3s context파일(k3s.yaml)을 표준경로(~/.kube/config)로 옮겨준다.
# 표준경로에 기존 context가 있다면, 수동으로 텍스트 편집하여 k3s 추가 
# context 파일의 소유자, 소유그룹이 실제 작업 user와 일치하면서 `chmod 600`을 하면 warning, permission denied 없이 사용가능
mkdir -p ~/.kube
sudo cat /etc/rancher/k3s/k3s.yaml > ~/.kube/config
# sudo chown myuser:myuser ~/.kube/config  # 파일 소유자&소유그룹 변경
sudo chmod 600 ~/.kube/config

# kubectl 설정
echo 'export KUBECONFIG=~/.kube/config' >> ~/.bashrc
echo 'alias k="kubectl"' >> ~/.bashrc
source ~/.bashrc
```

- k3s 독자 경로인 `/etc/rancher/k3s/k3s.yaml`를 써도 되지만, 향후 context 관리에 차질이 있으므로 비권장

### K3s Agent 설치 (Node)

노드(호스트)를 추가할 때 K3s Agent 설치

```sh
# K3s Agent 설치
# K3s Server와 동일한 설치 명령어 (환경변수 K3S_URL, K3S_TOKEN이 있으면 K3s Agent가 설치되는 구조)
export K3S_URL=https://{k3s_server_ip}:6443
export K3S_TOKEN={k3s_server_token}
curl -sfL https://get.k3s.io | sh -s - --docker  # 도커 연동

# K3s Agent 설치 (한 번에 입력)
curl -sfL https://get.k3s.io | K3S_URL=https://{k3s_server_ip}:6443 K3S_TOKEN={k3s_server_token} sh -

# {k3s_server_token}은 Server 측에서 조회
sudo cat /var/lib/rancher/k3s/server/node-token

# 설치 완료 후 Server측에서 연결 확인
kubectl get nodes
```

### 리셋

```sh
# 설정 및 클러스터 리셋
sudo systemctl stop k3s
sudo rm -rf /var/lib/rancher/k3s/
sudo systemctl start k3s
```

### 삭제 [[참고]](https://docs.k3s.io/installation/uninstall)

```sh
# k3s-server 삭제
/usr/local/bin/k3s-uninstall.sh

# k3s-agent 삭제
/usr/local/bin/k3s-agent-uninstall.sh
```

## 참고

- K3s 프로세스는 service daemon으로 실행됨
- K3s의 default Container Runtime은 containerd

## 기타 Command

```sh
# k3s 서브커맨드로 kubectl
k3s kubectl

# 직접 kubectl도 사용가능
# (kubectl->k3s로 bin파일 link되어있음)
kubectl
```

```sh
# k3s는 daemon으로 실행된다.
sudo k3s server

# node
sudo k3s agent
```

## 클러스터 구성시 필요한 내부 네트워크 인가[[참조]](https://docs.k3s.io/installation/requirements#inbound-rules-for-k3s-nodes)

- K8s 배포판 및 설정마다 조금씩 다름
- 여기선 K3s default 기준으로 최소 필요목록을 기술함
- 아래 포트를 모든 노드 간 방화벽 허용하면 됨
- **6443**
  - K8s API Server
  - kubectl이 controlplane에 접근하기 위한 용도
- **10250**
  - K8s kublet metrics
  - metrics 모니터링
- **8472**
  - K8s Pod-to-Pod
  - **UDP**
  - 클러스터 내부 Pod, ClusterIP 간 통신 허용
  - 노드를 넘나드는 트래픽이 있을 때 노드 간에 내부적으로 실제 사용되는 포트
- **30000-32767**
  - K8s NodePort
  - 클러스터 내부에서 NodeIP:NodePort로 통신시 허용
  - 클러스터 구성에 필수는 아니지만, 초기설정해놓으면 편리
  - 원래 클러스터 내부 통신용 IP:Port로는 ServiceName(ClusterIP):ServicePort를 쓰는 것이 정석이고, 이 때 물리적 노드를 건너가는 트래픽의 경우 UDP 8472포트가 사용된다. 하지만 특정 앱 구현에 따라 실제 서버의 NodeIP:NodePort로 통신하는 경우가 있다.
    - (e.g. bitnami/kafka에서 advertised.listeners로 통신시 실제 서버IP:Port를 이용하도록 설계됨)

## 클라우드(AWS EC2) 클러스터 구축 사례 (EKS 아님)

- 조건에 따라 다양한 네트워크 구성방법이 있겠으나, 일반적으로 다음과 같이 하는게 편하다.

### 클러스터 내부 통신

- 클러스터 내부 통신은 Private IP로 한다.
- 단일 VPC 내에 모든 노드가 있다면, `자기참조하는 보안그룹`을 생성하여 모든 노드에 등록한다. 자기참조 보안그룹이 등록된 노드들끼리는 전체 포트에 대하여 Private IP로 상호통신할 수 있다.
- 이후 클러스터 구성시 각 노드에서 K8s API server(:6443)를 가리키는 주소를 Private IP로 설정하면 된다.
- 개별 보안규칙을 자세히 설정할 수도 있겠으나, 향후 스케일링 등 유지보수 상황에서 매우 번거로워질 수 있기 때문에 지양한다.
- Public으로 클러스터 구성시 더 느리고, 비용들고, 보안적으로 좋지 않음

### 클러스터 외부 통신

- 소규모 클러스터
  - 각 노드에 할당된 Public IP를 이용
  - 모든 노드가 공유하는 공통보안그룹 사용할 것
- 대규모 or 스케일링이 잦은 클러스터
  - AWS NAT와 AWS LoadBalancer(LB)를 활용
    - `외부시스템과 연결시 IP기반 상호 인증/인가 등이 필요한데, 노드마다 이를 처리하기는 번거롭다. 큰 서비스 단위로 단일지점을 제공할 필요`가 있음.
    - 또한 직결되는 Public IP가 노출되지 않으므로 보안적으로도 더 우수
  - 인바운드
    - 외부시스템(Public) => `AWS LB(Public IP, DNS)` => 클러스터(Private IP)
    - EKS가 아니므로 LB 자동생성은 불가
    - AWS LB 수동 생성 후 K8s에서 LoadBalancer Service 배포시 LB 주소와 메타데이터를 적절히 입력하면 연동된다.
    - LB에는 개별 보안그룹 설정필요
    - `LB 단에서 SSL/TLS 인증 처리가 가능`함. ACM(AWS Certificate Manager) 활용시 무료발급/자동갱신까지 된다! 이렇게 하면 클러스터 내부에 배포되는 App. 레벨에서 인증서를 신경쓰지 않아도 된다.
  - 아웃바운드
    - 클러스터(Private IP) => `AWS NAT(Public IP)` => 외부시스템(Public)
    - NAT 개별 생성 후 연동

## 온프레미스 클러스터 구축 사례(K3s)

### 시작하기전에

- k3s에서 Pod 간 네트워크는 CNI(Container Network Interface) 표준을 기반으로 flannel이 대표적으로 활용되며, flannel은 VXLAN(Virtual eXtensible LAN) 오버레이 기술을 사용해
노드 간 물리적 네트워크 위에 가상 네트워크 터널을 만든 뒤, Pod들의 트래픽을 8472/UDP 터널 포트로 캡슐화해 각 노드 간 안전하게 전달한다.
- NIC(Network Interface Controller, 네트워크 인터페이스)
  - `ifconfig`했을 때 나오는 각 항목
- CNI(Container Network Interface)
  - CNCF에서 관리하는 컨테이너 통신을 위한 네트워크 표준 규격
- VXLAN
  - vpn과 비슷한 구조
- Flannel
  - K3s에서 default로 채택한 CNI 도구

### 인프라 환경

- 서버 7대
- eth0: 기본 사설망 IP
- eth1: 고대역폭망 IP
- 고대역폭이 필요한 시스템으로, 별도 망에 추가로 연결되었다.
- 각 서버마다 `다중 IP가 할당`된 상황이며, `별도 설정이 없을시 K3s는 기본 사설망(eth0)을 default로 선택`한다.
- 이를 `고대역폭망 IP(eth1)로 변경`하여 클러스터 내외부 트래픽이 송수신되도록 해야 함

### 설정 필요한 부분

- 각 노드에서 가리키는 K3s Api Server의 주소를 eth1로 설정(:6443에 관여)
- 각 노드의 internal ip를 eth1로 설정(:8472, :10250에 관여)
  - 각 노드가 클러스터 내부에서 자기자신을 인식하는 주소를 의미
  - 노드마다 다르게 실제 eth1으로 할당된 주소를 설정해줘야 함
- flannel의 NIC를 eth1로 설정(:8472에 관여)
  - 앞 과정에서 internal ip를 설정했음에도 불구하고 flannel이 eth0으로 트래픽전송 시도함
  - eth1을 수동선택 해줘야 함
- external ip
  - Service 리소스 배포시 eth1의 IP로 설정

### 설정방법

- k3s server, agent 설치단계에서 옵션설정으로 가능

```sh
# 설치 시 설정
curl -sfL https://get.k3s.io | sh -s - --docker --node-ip x.x.x.x --flannel-iface=eth1
```

- 이미 설치된 경우, systemd 서비스파일을 수정하는 방식으로도 변경가능
- 실행 명령어에서 옵션 추가 후 daemon-reload와 restart
  - /etc/systemd/systemd/k3s.service
  - /etc/systemd/systemd/k3s-agent.service
  - 그래도 신규설정이 반영되지 않을 수 있는데, 이 때는 각 논리적 노드를 삭제(kubectl delete node)하고 재등록해준다. 재등록은 서비스 restart하면 됨.

```service
# 서비스 파일 내 수정 할 부분
ExecStart=/usr/local/bin/k3s server --docker --node-ip x.x.x.x --flannel-iface=eth1
```

### 트러블 슈팅을 위한 기타 명령어

```sh
# 모든 NIC에 대해 8472 udp 포트 패킷 캡처
sudo tcpdump -i any udp port 8472 -vv

# NIC 'eth1'의 8472 udp 포트 패킷 캡처
sudo tcpdump -i eth1 udp port 8472 -vv

# internal ip 확인
kubectl get nodes -o wide
```
